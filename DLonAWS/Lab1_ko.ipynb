{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 2: 신경망을 사용하여 다층 퍼셉트론 모델 생성 및 실행\n",
    "\n",
    "이번 과제에서는 [Apache MXNet](https://mxnet.apache.org/) 과 같은 강력한 ML 라이브러리에서 나온 추상화된 방법을 사용하지 않고 신경망을 생성합니다. 하지만, MXNet의 몇몇 기본 함수를 사용하면 이러한 신경망을 쉽게 모델링할 수 있습니다.\n",
    "\n",
    "가장 단순한 신경망으로 쉽게 표현할 수 있는 **로지스틱 회귀 분석** 문제에 초점을 맞춥니다. 결과\\(레이블\\)가 알려진 모조 데이터 세트를 생성합니다. 필기 숫자의 28x28 흑백 사진을 반으로 자른 사진으로 구성된 [MNIST 데이터 세트](http://yann.lecun.com/exdb/mnist/)를 사용합니다.\n",
    "\n",
    "![](mnistdigits.gif)\n",
    "\n",
    "숫자를 감지하기 위해 4계층 신경망을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shift+Enter** 키를 눌러 셀을 실행합니다. 셀 실행이 완료되면 셀 좌측의 숫자가 **In [*]:** 에서 **In [1]** 로 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd\n",
    "print(\"Dependencies imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MXNet에서 GPU를 사용하여 지정하려면, 다음번 셀에서 다음 명령을 실행합니다. CPU를 대신 사용하려면 아래 셀을 실행하기 전에 CPU 줄의 주석 처리를 해제하고 GPU 줄을 주석 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a GPU with MXNet\n",
    "ctx = mx.gpu()\n",
    "\n",
    "# Use a CPU with MXNet\n",
    "# ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다층 퍼셉트론 신경망을 위해 [MNIST 이미지 데이터 세트](http://yann.lecun.com/exdb/mnist/)를 사용합니다. 데이터 세트를 다운로드하려면, 다음번 셀에서 다음을 입력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MNIST image dataset\n",
    "mnist = mx.test_utils.get_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망에 대한 파라미터를 정의하려면 다음번 셀에 다음 코드를 붙여넣습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of inputs: 1-dimensional input consisting of a single image (28x28 pixels)\n",
    "num_inputs = 784\n",
    "\n",
    "# Number of outputs: Number of outputs to be predicted by the network (digits 0-9)\n",
    "num_outputs = 10\n",
    "\n",
    "# Batch size: Number of images processed in a single batch\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터를 정의한 후에 데이터 세트를 교육 데이터와 테스트 데이터로 나눌 수 있습니다. Gluon API를 사용하여 DataLoader를  미니 배치로 데이터 세트를 반복합니다. DataLoader는 Dataset에서 샘플의 미니 배치를 생성하는 데 사용되며 이러한 배치를 반복하기 위한 편리한 반복기 인터페이스를 제공합니다. 컴퓨팅을 병렬로 수행할 수 있기 때문에 일반적으로 신경망을 통해 데이터의 미니 배치를 전달하는 것이 한 번에 단일 샘플보다 훨씬 효율적입니다.\n",
    "\n",
    "Gluon은 `mxnet.gluon.data.vision.MNIST`를 사용하여 MNIST 데이터 세트를 다운로드할 수 있는 편리한 API를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training data and test data\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype(np.float32)/255, label.astype(np.float32)\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 **숨겨진 뉴런 수** 및 **가중치 배율** 등의 유용한 파라미터를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden neurons\n",
    "num_hidden = 128\n",
    "\n",
    "# Weights scale\n",
    "weight_scale = .01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 계층에 대한 파라미터(**가중치** 및 **편중치**)를 할당하려면, 다음번 셀에서 다음 코드를 실행합니다.\n",
    "\n",
    "<i class=\"fas fa-comment\"></i> 다음 셀은 실행하는 데 시간이 다소 소요될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the first layer\n",
    "w_hd_1 = nd.random_normal(shape=(num_inputs, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_1 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째 계층에 대한 파라미터를 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the second layer\n",
    "w_hd_2 = nd.random_normal(shape=(num_hidden, num_hidden), scale=weight_scale, ctx=ctx)\n",
    "b_hd_2 = nd.random_normal(shape=num_hidden, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 계층에 대한 파라미터를 할당합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate weights and bias for the output layer\n",
    "w_output = nd.random_normal(shape=(num_hidden, num_outputs), scale=weight_scale, ctx=ctx)\n",
    "b_output = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기울기를 계산할 수 있도록 목록에 파라미터를 추가하려면, 다음번 셀에서 다음 코드를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parameters to calculate gradients\n",
    "params = [w_hd_1, b_hd_1, w_hd_2, b_hd_2, w_output, b_output]\n",
    "\n",
    "\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 뉴런으로 다중 계층 네트워크를 정의하면, 해당 네트워크는 선형 함수만 될 수 있습니다. 이는 다음의 이유 때문입니다: $\\hat{y} = X \\cdot W_1 \\cdot W_2 \\cdot W_2 = X \\cdot W_4 $ for $W_4 = W_1 \\cdot W_2 \\cdot W3$. 네트워크에서 비선형 속성을 포착하도록 하려면, 계층 끝에 활성화 함수를 추가해야 합니다. 이 경우 ReLU 활성화 함수를 사용할 수 있습니다.\n",
    "\n",
    "은닉층에 대한 ReLU 활성화 함수를 정의하려면, 다음번 셀에서 다음 코드를 실행합니다.\n",
    "\n",
    "Sigmoid나 Tanh 등의 다른 활성화 함수를 사용할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ReLU activation function for the hidden layer\n",
    "def relu(X):\n",
    "    return nd.maximum(X, nd.zeros_like(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax output\n",
    "\n",
    "네트워크의 출력에서 예측은 숫자 0\\~9를 예측하는 배열이 됩니다. 출력 계층을 위한 Softmax 활동 함수는 이미지가 특정 클래스에 속하는 확률을 제공합니다. 예를 들어, 배열의 첫 숫자가 0.65라면, 숫자가 0일 확률이 65%라는 뜻입니다.\n",
    "\n",
    "Softmax 확률을 새 손실 함수에 전달하는 대신, yhat_linear를 전달하고 softmax_cross_entropy 손실 함수 내에서 softmax와 로그를 모두 한 번에 계산합니다. 이 함수는 LogSumExp 트릭([LogSumExp on Wikipedia](https://en.wikipedia.org/wiki/LogSumExp))와 같은 스마트한 기능을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a softmax action function for the output layer\n",
    "def softmax_cross_entropy(yhat_linear, y):\n",
    "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 3: 인공 신경망 모델 정의\n",
    "\n",
    "이번 과제에서는 인공 신경망, 가중치와 편중치를 학습하기 위한 최적화 도구 및 모델 수행 방식을 평가하는 평가 지표를 정의합니다.\n",
    "\n",
    "모델을 정의하려면, 새 셀에서 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "def net(X):\n",
    "\n",
    "    #  Compute the first hidden layer\n",
    "    h1_linear = nd.dot(X, w_hd_1) + b_hd_1\n",
    "    h1 = relu(h1_linear)\n",
    "\n",
    "    #  Compute the second hidden layer\n",
    "    h2_linear = nd.dot(h1, w_hd_2) + b_hd_2\n",
    "    h2 = relu(h2_linear)\n",
    "\n",
    "    #  Compute the output layer\n",
    "    yhat_linear = nd.dot(h2, w_output) + b_output\n",
    "    return yhat_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반환된 변수는 여전히 Softmax 함수가 적용되지 않은 선형 변수입니다. 해당 함수는 역 전달 중 발생할 수 있는 수치 안정성 문제를 방지하기 위해 softamx_cross_entropy에 직접 적용됩니다.\n",
    "\n",
    "가중치와 편중치를 학습하기 위한 최적화 도구를 정의하려면 새 셀에서 다음을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer\n",
    "def SGD(params, lr):\n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 지표를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metric\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    numerator = 0.\n",
    "    denominator = 0.\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        numerator += nd.sum(predictions == label)\n",
    "        denominator += data.shape[0]\n",
    "    return (numerator / denominator).asscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과제 4: 교육 루프 및 모델 평가 실행\n",
    "\n",
    "이 작업에서는 교육 루프를 실행하고 모델을 평가합니다.\n",
    "\n",
    "교육 루프 실행을 위한 파라미터를 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs: Iterations over the full network\n",
    "epochs = 10\n",
    "\n",
    "# Learning rate: Speed at which the network learns\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define a smooth constant for the moving loss\n",
    "smoothing_constant = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공 신경망 모델을 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network model\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, 784))\n",
    "        label = label.as_in_context(ctx)\n",
    "        label_one_hot = nd.one_hot(label, 10)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label_one_hot)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate)\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Output:**\n",
    "\n",
    "Epoch 0. Loss: 0.462392371464, Train_acc 0.8805, Test_acc 0.8831 \n",
    "\n",
    "Epoch 1. Loss: 0.285959471388, Train_acc 0.919967, Test_acc 0.9194\n",
    "\n",
    "Epoch 2. Loss: 0.198550129106, Train_acc 0.94725, Test_acc 0.9499\n",
    "\n",
    "Epoch 3. Loss: 0.159744916748, Train_acc 0.9602, Test_acc 0.958\n",
    "\n",
    "Epoch 4. Loss: 0.125638222475, Train_acc 0.967033, Test_acc 0.9619\n",
    "\n",
    "Epoch 5. Loss: 0.101477091803, Train_acc 0.97465, Test_acc 0.9689\n",
    "\n",
    "Epoch 6. Loss: 0.0901461152782, Train_acc 0.976233, Test_acc 0.9693\n",
    "\n",
    "Epoch 7. Loss: 0.0763436301528, Train_acc 0.98115, Test_acc 0.9737\n",
    "\n",
    "Epoch 8. Loss: 0.0693615894988, Train_acc 0.9841, Test_acc 0.9745\n",
    "\n",
    "Epoch 9. Loss: 0.0573878861228, Train_acc 0.985933, Test_acc 0.9739\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화\n",
    "\n",
    "테스트 세트에서 무작위 데이터 요소 몇 개를 선택하여 예측과 함께 시각화합니다. 정량적으로는 모델이 더 정확하지만 아래의 이유 때문에 결과를 시각화하는 것이 좋습니다.\n",
    "* 코드가 실제로 작동하는지 확인 가능\n",
    "* 모델이 어떤 종류의 실수를 저지르는 경향이 있는지에 대한 직관 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the function to do prediction\n",
    "def model_predict(net,data):\n",
    "    output = net(data)\n",
    "    return nd.argmax(output, axis=1)\n",
    "\n",
    "samples = 10\n",
    "\n",
    "mnist_test = mx.gluon.data.vision.MNIST(train=False, transform=transform)\n",
    "\n",
    "# let's sample 10 random data points from the test set\n",
    "sample_data = mx.gluon.data.DataLoader(mnist_test, samples, shuffle=True)\n",
    "for i, (data, label) in enumerate(sample_data):\n",
    "    data = data.as_in_context(ctx)\n",
    "    im = nd.transpose(data,(1,0,2,3))\n",
    "    im = nd.reshape(im,(28,10*28,1))\n",
    "    imtiles = nd.tile(im, (1,1,3))\n",
    "    \n",
    "    plt.imshow(imtiles.asnumpy())\n",
    "    plt.show()\n",
    "    pred=model_predict(net,data.reshape((-1,784)))\n",
    "    print('model predictions are:', pred)\n",
    "    print('true labels :', label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보이지 않는 데이터에 대한 모델의 작동 방식을 확인하려면 새 셀에서 다음을 실행하여 Jupyter 노트북에 그릴 수 있는 캔버스가 포함된 html 페이지를 다운로드합니다.\n",
    "\n",
    "Jupyter 노트북에는 매직 명령어\\(Magic Command\\)가 내장되어 있어 Jupyter 노트북 셀에서 Bash 명령어 및 기타 명령어를 실행할 수 있습니다. 자세한 내용은 [Magic Commands](http://ipython.readthedocs.io/en/stable/interactive/magics.html)를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget http://us-west-2-tcdev.s3.amazonaws.com/<% KAIZEN_S3_LAB_PREFIX %>/scripts/mnist.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 셀을 실행하여 모델을 평가하기 위한 HTML 캔버스를 생성합니다.\n",
    "\n",
    "마우스를 사용하여 표시된 사각형 안에 숫자를 그린 다음 **Classify**를 클릭합니다.\n",
    "\n",
    "마우스로 사각형 안에 숫자를 적고 Classify 버튼을 눌러 분류 결과를 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HTML canvas to evaluate the model\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "def classify(img):\n",
    "    img = base64.b64decode(img[len('data:image/png;base64,'):])\n",
    "    img = cv2.imdecode(np.fromstring(img, np.uint8),-1)\n",
    "    img = cv2.resize(img[:,:,3], (28,28))\n",
    "    img = nd.array(img).as_in_context(ctx).reshape((-1, 784)).astype(np.float32)/255\n",
    "    return int(nd.argmax(net(img), axis=1).asnumpy()[0])\n",
    "\n",
    "HTML(filename = \"mnist.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 도전 과제\n",
    "\n",
    "- 네트워크 크기를 5개의 계층으로 변경해 보십시오.\n",
    "- 숨겨진 단위 수를 256으로 변경해 보십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 완료\n",
    "\n",
    "축하합니다! 본 실습을 완료했습니다. 실습 환경을 정리하려면 다음을 수행하십시오.\n",
    "\n",
    "- 노트북 파일을 닫습니다.\n",
    "- **Quit** 을 클릭하여 Jupyter 노트북에서 로그아웃한 다음 탭을 닫습니다.\n",
    "- AWS Management Console에서 로그 아웃하려면 콘솔 맨 위에 있는 **awsstudent**을 클릭한 후 **Sign Out**을 클릭합니다.\n",
    "- **End Lab** 을 클릭하여 Qwiklabs에서 실습 세션을 종료합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}